\documentclass{exam}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{ragged2e}
\usepackage{lmodern}
\usepackage{tcolorbox}
\usepackage{hyperref}

\pagestyle{empty}
\renewcommand{\theenumi}{\alph{enumi}}
\renewenvironment{proof}{{\noindent\itshape\ignorespaces}}{{\hfill$\qed$\\}}

\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\begin{center}
    \textbf{\Large Exercises 3.15 }
\end{center}

\section*{Exercise 3.15.1}
Let $p, \ p_i, \ q, \ q_i$ be density functions on $\mathbb{R}$ and $\alpha \in \mathbb{R}$. Show that the cross-entropy satificies the following properties:
\begin{enumerate}
    \item $S(p_1 + p_2,q) = S(p_1,q) + S(p_2,q)$;
    \item $S(\alpha p,q) = \alpha S(p,q) = S(p,q^{\alpha})$;
    \item $S(p, q_1 q_2) = S(p,q_1) + S(p,q_2)$.
\end{enumerate}

\section*{Exercise 3.15.2}
Show that the cross entropy satifies the following inequality\\
\begin{equation*}
    S(p,q) \geq 1 - \displaystyle\int p(x) q(x) d x
\end{equation*}    

\section*{Exercise 3.15.3}
Let $p$ a fixed density. Show that the symetric relative entropy \\
\begin{equation*}
    D_{KL}(p \lVert q) +  D_{KL}(q \lVert p )
\end{equation*}\\
reaches its minimum for $p = q$, and the minimum is equal to zero. 

\section*{Exercise 3.15.4}
Consider two exponential densities, $p_1 = \xi^1 e^{\xi^1 x}$ and $p_2 = \xi^2 e^{\xi^2 x}$, $x \geq 0$.
\begin{enumerate}
    \item Show that $D_{KL}(p_1 \lVert p_2) = \displaystyle \frac{\xi^2}{\xi^1} - \text{ln}\frac{\xi^2}{\xi^1} - 1$.  
    \item Verify $D_{KL}(p_1 \lVert p_2) \neq D_{KL}(p_2 \lVert p_2 )$.
    \item Show that the triangle inequality doesn't hold for three arbitrary densities. 
\end{enumerate}    

\section*{Exercise 3.15.5}
Let $X$ be a discrete random variable. Show the inequality\\
\begin{equation*}
    H(X) \geq 0.
\end{equation*}\\

\section*{Exercise 3.15.5}
Prove that if $p$ and $q$ are the densities of two discrete random variables, then $D_{KL}(p \lVert q ) \leq S(p,q)$
\section*{Exercise 3.15.7}
We asume the target variable $Z$ is $\mathcal{E}$-mesurable. What is mean squared error function in this case?

\section*{Exercise 3.15.8}
Asume that a neural network has an input-output function $f_{w,b}$ linear in $w$ and $b$. Show that the cost function $(3.3.1)$ reaches its minimum for a 
unique pair $(w^{\star},b^{\star})$, which can be computed explicitly. 

\section*{Exercise 3.15.9}
Show that the Shannon entropy can be retrived from the Reyni entropy as\\
\begin{equation*}
    H(p) = \lim_{\alpha \to 1} H_{\alpha}(x).
\end{equation*}\\

\section*{Exercise 3.15.10}
Let $\phi_\sigma(x) = \displaystyle \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{t^2}{2\sigma^2}}$. Consider the convolution 
operation $(f \ast g)(x) :=  \displaystyle\int f(t) g(x - t) dt $.
\begin{enumerate}
    \item Show that $\phi_{\sigma} \ast \phi_{\sigma} = \phi_{\sigma\sqrt{2}}$;
    \item Find $\phi_{\sigma} \ast \phi_{\sigma^{\prime}}$ in the case $\sigma \neq \sigma^{\prime}$.
\end{enumerate}

\section*{Exercise 3.15.11}
Consider two probability densitie, $p(x)$ and $q(x)$. The Cauchy-Schwartz divergence is defined by 
\begin{equation*}
    D_{CS}(p,q) := -\text{ln}(\displaystyle\frac{\int p(x) q(x) d x}{\sqrt{\int p(x)^2 d x } \sqrt{\int q(x)^2 d x }})
\end{equation*}.\\
Show the following: 
\begin{enumerate}
    \item $D_{CS}(p,q) = 0$ if and only if $p = q$;
    \item $D_{CS}(p,q) \geq 0$;
    \item $D_{CS}(p,q) = D_{CS}(q,p)$;
    \item $D_{CS}(p,q) = \displaystyle-\text{ln}\int pq d x  - \frac{1}{2}H_2(p) - \frac{1}{2}H_2(q)$, where $H_2(\cdot)$ denotes the quadratic Reyni entropy.
\end{enumerate}

\section*{Exercise 3.15.12}
\begin{enumerate}
    \item Show that for any function $f \in L^{1}[0,1]$ we have the inequality $\lVert \text{tanh}(f) \rVert_{1} \leq \lVert f \rVert_{1}$.
    \item Show that for any function $f \in L^{2} [0,1]$ we have the inequality $\lVert \text{tanh} \rVert_{2} \leq \lVert f \rVert_{2}$.
\end{enumerate}

\section*{Exercise 3.15.13}
Consider two distributions on the sample space $\mathcal{X} = \{ x_1, x_2\}$ given by 
\begin{equation*}
    p = \begin{pmatrix}
        x_1 & x_2 \\
        \frac{1}{2} & \frac{1}{2}
        \end{pmatrix}, \ q = \begin{pmatrix}
            x_1 & x_2\\
            \frac{1}{2} & \frac{2}{3}
            \end{pmatrix}
\end{equation*}
Consider the function $\phi: \mathcal{X} \to \mathbb{R}^2$ defined by $\phi(x_1) = (0,1)$ $\phi(x_2) = (1,0)$. Find the maximum mean discrepancy between $p$ and $q$.
% % Solutions to other exercises in Chapter X...

\newpage

\begin{center}    
    \section*{SOLUTIONS}
\end{center}

\subsection*{3.15.1 (a)}
The claim follows from the linearity of the integral operator. In symbols we have:\\
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            S(p_1 + p_2,q) &= -\displaystyle\int_{\mathbb{R}} (p_1(x) + p_2(x)) \text{ln}q(x) dx = - \displaystyle\int_{\mathbb{R}} p_1(x) \text{ln}q(x) dx - \displaystyle\int_{\mathbb{R}} p_2(x) \text{ln}q(x) dx \\
            &=S(p_1,q) + S(p_2,q).
        \end{aligned}    
    \end{equation*}
\end{proof}

\subsection*{3.15.1 (b)}
From the linearity of the integral operator, and the property $c \ \text{ln}(x) = \text{ln}(x^{c})$ we have:
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            S(\alpha p,q) &= -\displaystyle\int_{\mathbb{R}}   \alpha p(x) \text{ln}q(x) dx = - \alpha \int_{\mathbb{R}} p(x) \text{ln}q(x) dx = \alpha S(p,q) \\
            &= -\displaystyle\int_{\mathbb{R}}   \alpha p(x) \text{ln}q(x) dx = -\displaystyle\int_{\mathbb{R}} p(x) \text{ln}q(x)^{\alpha} dx = S(p,q^{\alpha}).
        \end{aligned}    
    \end{equation*}
\end{proof}


\subsection*{3.15.1 (c)}
Using the addition identity for the logarithms we get: 
\begin{proof}
\begin{equation*}
    \begin{aligned}
        S(p,q_1 q_2) &= -\displaystyle\int_{\mathbb{R}} p(x) \text{ln}q_1(x) q_2(x) dx = - \int_{\mathbb{R}} p(x) \text{ln}q_1(x) dx - \int_{\mathbb{R}} p(x) \text{ln}q_2(x) \\
        &= S(p,q_1) + S(p,q_2).
    \end{aligned}    
\end{equation*}
\end{proof}

\subsection*{3.15.2}
By the inequality $\text{ln}(x) \leq x - 1, \ \forall x \in \mathbb{R}^{+}$, and the definition of cross-entropy follows:\\
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            S(p,q) &= -\displaystyle\int_{\mathbb{R}}   p(x) \text{ln}q(x) dx \geq - \int_{\mathbb{R}} p(x)(q(x) - 1) dx \\
            &\geq -\displaystyle\int_{\mathbb{R}} - p(x) dx  -\displaystyle\int_{\mathbb{R}} p(x) q(x) dx =  1 - \displaystyle\int_{\mathbb{R}} p(x) q(x) dx.
        \end{aligned}    
    \end{equation*}
\end{proof}

\subsection*{3.15.3}
Noting that $D_{KL}(p \lVert q) +  D_{KL}(q \lVert p ) = 2 D_{JS}(p \lVert q ) $ is obvious then, that\\
\begin{proof}
    $\text{min} \ \{ D_{KL}(p \lVert q) +  D_{KL}(q \lVert p ) \} \Leftrightarrow \text{min} \ \{ D_{JS}(p \lVert q )\}$. \\
    \\
    From proposition $3.17.1$(p. $51$) we know $ 0 = D_{JS}(p \lVert p) = D_{JS}(q \lVert q) \leq D_{JS}(p \lVert q)$, i.e the minimum 0 is attained when 
    $p=q$.
\end{proof}

\subsection*{3.15.4 (a)}
By direct calculation we find:
\begin{proof}
    \begin{equation*}
        \begin{aligned}
           D_{KL}(p_1 \lVert p_2) &= S(p_1,p_2) - H(p_1) = -\displaystyle\int_{\mathbb{R}}  \xi^1 e^{-\xi^1 x} \text{ln}(\xi^2e^{-\xi^2 x}) dx  -\displaystyle\int_{\mathbb{R}}  \xi^1 e^{-\xi^1 x} \text{ln}(\xi^1e^{-\xi^1 x}) \\
           &= -\displaystyle\int_{\mathbb{R}}  \xi^1 e^{-\xi^1 x} \text{ln}(\xi^2) dx + \displaystyle\int_{\mathbb{R}}  \xi^{1} e^{-\xi^1 x} \xi^{2} x d x + \displaystyle\int_{\mathbb{R}} \xi^1 e^{-\xi^1 x} \text{ln}(\xi^1) dx  - \displaystyle\int_{\mathbb{R}} \xi^{1}e^{-\xi^{1}x} \xi^{1} x d x \\
           &= - (\text{ln}(\xi^{2}) - \text{ln}(\xi^{1}))\displaystyle\int_{\mathbb{R}}  \xi^1 e^{-\xi^1 x} dx + (\xi^{1} - \xi^{2})\displaystyle\int_{\mathbb{R}} \xi^1 x e^{-\xi^1 x} dx \\ 
           &= - (\text{ln}(\xi^{2}) - \text{ln}(\xi^{1}))\mathbb{E}_{\tiny X \sim exp(\xi^{1})}\left[1\right] + (\xi^{2} - \xi^{1})\mathbb{E}_{\tiny X \sim exp(\xi^{1})}\left[ X \right] = - \text{ln}\frac{\xi^{2}}{\xi^{1}} + (\xi^{2} - \xi^{1})\frac{1}{\xi^{1}} \\
           &= - \text{ln}\frac{\xi^{2}}{\xi^{1}} + \frac{\xi^{2}}{\xi^{1}} - 1
        \end{aligned}
    \end{equation*}
\end{proof}

%At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos 
%dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, 
%id est laborum et dolorum fuga.
%\begin{proof}
    %\begin{equation*}     
        %\begin{aligned}
            %\sigma^{\prime\prime\prime}(x) &= \frac{d}{ d x} \frac{e^{x}-e^{2x}}{(1 + e^{x})^3} = \frac{(e^x - 2e^{2x})(1 + e^x)^3 - 3(1 + e^x)^2 e^{x}(e^x - e^{2x})}{(1 + e^x)^6}\\
            %&= \frac{e^{x} \{ 1 - 4e^x + e^{2x} \}(1 + e^x)^2}{(1 + e^x)^{6}} = \frac{e^{x} \{ 1 - 4e^x + e^{2x} \}}{(1 + e^x)^{4}}
        %\end{aligned}
%\end{equation*}
%\end{proof}
%\subsection*{x.y.2 (a)}
%t harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est 
%eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, 
%omnis dolor repellendus. 
\end{document}