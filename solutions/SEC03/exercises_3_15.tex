\documentclass{exam}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{ragged2e}
\usepackage{lmodern}
\usepackage{tcolorbox}
\usepackage{hyperref}

\pagestyle{empty}
\renewcommand{\theenumi}{\alph{enumi}}
\renewenvironment{proof}{{\noindent\itshape\ignorespaces}}{{\hfill$\qed$\\}}

\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\begin{center}
    \textbf{\Large Exercises 3.15 }
\end{center}

\section*{Exercise 3.15.1}
Let $p, \ p_i, \ q, \ q_i$ be density functions on $\mathbb{R}$ and $\alpha \in \mathbb{R}$. Show that the cross-entropy satificies the following properties:
\begin{enumerate}
    \item $S(p_1 + p_2,q) = S(p_1,q) + S(p_2,q)$;
    \item $S(\alpha p,q) = \alpha S(p,q) = S(p,q^{\alpha})$;
    \item $S(p, q_1 q_2) = S(p,q_1) + S(p,q_2)$.
\end{enumerate}

\section*{Exercise 3.15.2}
Show that the cross entropy satifies the following inequality\\
\begin{equation*}
    S(p,q) \geq 1 - \displaystyle\int p(x) q(x) d x
\end{equation*}    

\section*{Exercise 3.15.3}
Let $p$ a fixed density. Show that the symetric relative entropy \\
\begin{equation*}
    D_{KL}(p \lVert q) +  D_{KL}(q \lVert p )
\end{equation*}\\
reaches its minimum for $p = q$, and the minimum is equal to zero. 

\section*{Exercise 3.15.4}
Consider two exponential densities, $p_1 = \xi^1 e^{\xi^1 x}$ and $p_2 = \xi^2 e^{\xi^2 x}$, $x \geq 0$.
\begin{enumerate}
    \item Show that $D_{KL}(p_1 \lVert p_2) = \displaystyle \frac{\xi^2}{\xi^1} - \text{ln}\frac{\xi^2}{\xi^1} - 1$.  
    \item Verify $D_{KL}(p_1 \lVert p_2) \neq D_{KL}(p_2 \lVert p_1 )$.
    \item Show that the triangle inequality doesn't hold for three arbitrary densities. 
\end{enumerate}    

\section*{Exercise 3.15.5}
Let $X$ be a discrete random variable. Show the inequality\\
\begin{equation*}
    H(X) \geq 0.
\end{equation*}\\

\section*{Exercise 3.15.6}
Prove that if $p$ and $q$ are the densities of two discrete random variables, then $D_{KL}(p \lVert q ) \leq S(p,q)$

\section*{Exercise 3.15.7}
We asume the target variable $Z$ is $\mathcal{E}$-measurable. What is mean squared error function in this case?

\section*{Exercise 3.15.8}
Asume that a neural network has an input-output function $f_{w,b}$ linear in $w$ and $b$. Show that the cost function $(3.3.1)$ reaches its minimum for a 
unique pair $(w^{\star},b^{\star})$, which can be computed explicitly. 

\section*{Exercise 3.15.9}
Show that the Shannon entropy can be retrived from the Reyni entropy as\\
\begin{equation*}
    H(p) = \lim_{\alpha \to 1} H_{\alpha}(x).
\end{equation*}\\

\section*{Exercise 3.15.10}
Let $\phi_\sigma(x) = \displaystyle \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{t^2}{2\sigma^2}}$. Consider the convolution 
operation $(f \ast g)(x) :=  \displaystyle\int f(t) g(x - t) dt $.
\begin{enumerate}
    \item Show that $\phi_{\sigma} \ast \phi_{\sigma} = \phi_{\sigma\sqrt{2}}$;
    \item Find $\phi_{\sigma} \ast \phi_{\sigma^{\prime}}$ in the case $\sigma \neq \sigma^{\prime}$.
\end{enumerate}

\section*{Exercise 3.15.11}
Consider two probability densities, $p(x)$ and $q(x)$. The Cauchy-Schwartz divergence is defined by 
\begin{equation*}
    D_{CS}(p,q) := -\text{ln}(\displaystyle\frac{\int p(x) q(x) d x}{\sqrt{\int p(x)^2 d x } \sqrt{\int q(x)^2 d x }})
\end{equation*}\\
Show the following: 
\begin{enumerate}
    \item $D_{CS}(p,q) = 0$ if and only if $p = q$;
    \item $D_{CS}(p,q) \geq 0$;
    \item $D_{CS}(p,q) = D_{CS}(q,p)$;
    \item $D_{CS}(p,q) = \displaystyle-\text{ln}\int pq d x  - \frac{1}{2}H_2(p) - \frac{1}{2}H_2(q)$, where $H_2(\cdot)$ denotes the quadratic Reyni entropy.
\end{enumerate}

\section*{Exercise 3.15.12}
\begin{enumerate}
    \item Show that for any function $f \in L^{1}[0,1]$ we have the inequality $\lVert \text{tanh}(f) \rVert_{1} \leq \lVert f \rVert_{1}$.
    \item Show that for any function $f \in L^{2} [0,1]$ we have the inequality $\lVert \text{tanh}(f) \rVert_{2} \leq \lVert f \rVert_{2}$.
\end{enumerate}

\section*{Exercise 3.15.13}
Consider two distributions on the sample space $\mathcal{X} = \{ x_1, x_2\}$ given by 
\begin{equation*}
    p = \begin{pmatrix}
        x_1 & x_2 \\
        \frac{1}{2} & \frac{1}{2}
        \end{pmatrix}, \ q = \begin{pmatrix}
            x_1 & x_2\\
            \frac{1}{2} & \frac{2}{3}
            \end{pmatrix}
\end{equation*}
Consider the function $\phi: \mathcal{X} \to \mathbb{R}^2$ defined by $\phi(x_1) = (0,1)$ $\phi(x_2) = (1,0)$. Find the maximum mean discrepancy between $p$ and $q$.
% % Solutions to other exercises in Chapter X...

\newpage

\begin{center}    
    \section*{SOLUTIONS}
\end{center}

\subsection*{3.15.1 (a)}
The claim follows from the linearity of the integral operator. In symbols we have:\\
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            S(p_1 + p_2,q) &= -\displaystyle\int_{\mathbb{R}} (p_1(x) + p_2(x)) \text{ln}q(x) dx = - \displaystyle\int_{\mathbb{R}} p_1(x) \text{ln}q(x) dx - \displaystyle\int_{\mathbb{R}} p_2(x) \text{ln}q(x) dx \\
            &=S(p_1,q) + S(p_2,q).
        \end{aligned}    
    \end{equation*}
\end{proof}

\subsection*{3.15.1 (b)}
From the linearity of the integral operator, and the property $c \ \text{ln}(x) = \text{ln}(x^{c})$ we have:
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            S(\alpha p,q) &= -\displaystyle\int_{\mathbb{R}}   \alpha p(x) \text{ln}q(x) dx = - \alpha \int_{\mathbb{R}} p(x) \text{ln}q(x) dx = \alpha S(p,q) \\
            &= -\displaystyle\int_{\mathbb{R}}   \alpha p(x) \text{ln}q(x) dx = -\displaystyle\int_{\mathbb{R}} p(x) \text{ln}q(x)^{\alpha} dx = S(p,q^{\alpha}).
        \end{aligned}    
    \end{equation*}
\end{proof}


\subsection*{3.15.1 (c)}
Using the addition identity for the logarithm, we get: 
\begin{proof}
\begin{equation*}
    \begin{aligned}
        S(p,q_1 q_2) &= -\displaystyle\int_{\mathbb{R}} p(x) \text{ln}q_1(x) q_2(x) dx = - \int_{\mathbb{R}} p(x) \text{ln}q_1(x) dx - \int_{\mathbb{R}} p(x) \text{ln}q_2(x) \\
        &= S(p,q_1) + S(p,q_2).
    \end{aligned}    
\end{equation*}
\end{proof}

\subsection*{3.15.2}
By the inequality $\text{ln}(x) \leq x - 1, \ \forall x \in \mathbb{R}^{+}$, and the definition of cross-entropy follows:\\
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            S(p,q) &= -\displaystyle\int_{\mathbb{R}}   p(x) \text{ln}q(x) dx \geq - \int_{\mathbb{R}} p(x)(q(x) - 1) dx \\
            &\geq -\displaystyle\int_{\mathbb{R}} - p(x) dx  -\displaystyle\int_{\mathbb{R}} p(x) q(x) dx =  1 - \displaystyle\int_{\mathbb{R}} p(x) q(x) dx.
        \end{aligned}    
    \end{equation*}
\end{proof}

\subsection*{3.15.3}
From proposition $3.5.1$ follows that $D_{KL}(p \lVert q) \geq 0$, $D_{KL}(q \lVert p ) \geq 0$, then $ D_{KL}(p \lVert q) +  D_{KL}(q \lVert p ) \geq 0$. Clearly the value 
$0$ is a minimum. Let's now prove that this minimum is attained when $p=q$.It is well known from the cross-entropy definition $S(p,p) = H(p)$ and $S(q,q) = H(q)$ then:\\
\\
\begin{proof}
    $D_{KL}(p \lVert q) = D_{KL}(p \lVert p) = S(p,p) - H(p) = 0$ and $D_{KL}(q \lVert p) = D_{KL}(q \lVert q) = S(q,q) - H(q) = 0$, which in turn imply $D_{KL}(p \lVert q) + D_{KL}(q \lVert p) = 0 \text{ .}$
\end{proof}

\subsection*{3.15.4 (a)}
By direct calculation we find:
\begin{proof}
    \begin{equation*}
        \begin{aligned}
           D_{KL}(p_1 \lVert p_2) &= S(p_1,p_2) - H(p_1) = -\displaystyle\int_{\mathbb{R}}  \xi^1 e^{-\xi^1 x} \text{ln}(\xi^2e^{-\xi^2 x}) dx  -\displaystyle\int_{\mathbb{R}}  \xi^1 e^{-\xi^1 x} \text{ln}(\xi^1e^{-\xi^1 x}) \\
           &= -\displaystyle\int_{\mathbb{R}}  \xi^1 e^{-\xi^1 x} \text{ln}(\xi^2) dx + \displaystyle\int_{\mathbb{R}}  \xi^{1} e^{-\xi^1 x} \xi^{2} x d x + \displaystyle\int_{\mathbb{R}} \xi^1 e^{-\xi^1 x} \text{ln}(\xi^1) dx  - \displaystyle\int_{\mathbb{R}} \xi^{1}e^{-\xi^{1}x} \xi^{1} x d x \\
           &= - (\text{ln}(\xi^{2}) - \text{ln}(\xi^{1}))\displaystyle\int_{\mathbb{R}}  \xi^1 e^{-\xi^1 x} dx + (\xi^{2} - \xi^{1})\displaystyle\int_{\mathbb{R}} \xi^1 x e^{-\xi^1 x} dx \\ 
           &= - (\text{ln}(\xi^{2}) - \text{ln}(\xi^{1}))\mathbb{E}_{\tiny X \sim exp(\xi^{1})}\left[1\right] + (\xi^{2} - \xi^{1})\mathbb{E}_{\tiny X \sim exp(\xi^{1})}\left[ X \right] = - \text{ln}\frac{\xi^{2}}{\xi^{1}} + (\xi^{2} - \xi^{1})\frac{1}{\xi^{1}} \\
           &= - \text{ln}\frac{\xi^{2}}{\xi^{1}} + \frac{\xi^{2}}{\xi^{1}} - 1
        \end{aligned}
    \end{equation*}
\end{proof}

\subsection*{3.15.4 (b)}
Suppose the equality $D_{KL}(p \lVert p) = D_{KL}(q \lVert p)$ holds and $\xi^{1} \neq \xi^{2}$, then from exercise 3.14.4.a it follows: \\
$- \text{ln}\frac{\xi^{2}}{\xi^{1}} + \frac{\xi^{2}}{\xi^{1}} - 1 = - \text{ln}\frac{\xi^{1}}{\xi^{2}} + \frac{\xi^{1}}{\xi^{2}} - 1 \implies \frac{\xi^{2}}{\xi^{1}} = \frac{\xi^{1}}{\xi^{2}}$. The later implies 
$\frac{\xi^{1}}{\xi^{2}} = 1$ or equivalently $\xi^{1} = \xi^{2}$, which is a contradiction. 

\subsection*{3.15.4 (c)}
Let $p_1 = exp(2)$, $p_2 = exp(3)$, $p_3 = exp(4)$. Suppose the triangle inequality holds for these three arbitary exponential distributions. This is:\\
$D_{KL}(p_1 \lVert p_3) \leq D_{KL}(p_1 \lVert p_2) + D_{KL}(p_2 \lVert p_3)$. By exercise $3.15.4.b$ we would have:
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            D_{KL}(p_1 \lVert p_3) = \frac{4}{2} - \text{ln}\frac{4}{2} - 1  &\leq D_{KL}(p_1 \lVert p_2) + D_{KL}(p_2 \lVert p_3) = \frac{3}{2} - \text{ln}\frac{3}{2} - 1 + \frac{4}{3} - \text{ln}\frac{4}{3} - 1\\
            2 &\leq \frac{3}{2} + \frac{4}{3} -1  = \frac{17}{6} - 1 = \frac{11}{6} = \frac{12}{6} - \frac{1}{6} = 2 - \frac{1}{6} \text{ (contradiction!)}
        \end{aligned}
    \end{equation*}
\end{proof}

\subsection*{3.15.5 (a)}
Given that $p(x)$ is a distribution, it follows that $p(X)$  as a r.v satisffies the inequality $0 \leq p(X) \leq 1$. This means $p(x) \leq 1, \forall x \in \text{sup}\left( X \right)$. Taking natural logs on both sides of the inequality $p(x) \leq 1$ and multiplying by $-1$, we obtain: \ $\text{ln}p(x) \geq 0$; Multiplying by $p(x)$ and summing over the support of X, we get:\\
\begin{proof}
    \begin{equation*}
        \begin{aligned}
             \mathbb{E}\left[ - \text{ln}p(X) \right] = H(X) = \displaystyle \sum_{\tiny x \in \text{sup}\left( X \right)}-p(x) \ \text{ln}(p(x)) \geq 0 \text{.}
        \end{aligned}
    \end{equation*}
\end{proof}

\subsection*{3.15.6}
This is an inmediate consecuence of exercise $3.15.5$. Indeed, we have:\\
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            D_{KL}(p \lVert q) = S(p,q) - H(p) \leq S(p,q) - 0  \leq S(p,q) \text{.}
        \end{aligned}
    \end{equation*}
\end{proof}
\subsection*{3.15.7}
If the target variable $Z$ happens to be $\mathcal{E}-$measurable, then $Y$ is independent of the sigma algebra $\mathcal{E}$. From this follows that $C(\omega, b) = \text{d}(Z,Y)^2 = \mathbb{E} \left[ (Z - \mathbb{E}\left[ Z |\mathcal{E} \right] )^2\right]  = \mathbb{E} \left[ (Z - Z)^2\right] = 0$.

\subsection*{3.15.8}
In this case $ \displaystyle f_{\tiny \bold{\omega}, b}(\bold{x}) = \bold{\omega} \cdot \bold{x} + b$, definied on a compact subset of $\mathbb{R}^n$. Therefore, the cost function is given by: \\[0.25 em]
$C(\bold{\omega}, b) := \displaystyle \sum_{\tiny 0 \leq i\leq n } ( \bold{\omega} \cdot \bold{x}^{i} + b - \phi(\bold{x}^{i}))^{2}$. Obviously we have $0 \leq C(\bold{\omega}, b)$. This means the function attains such minimum inside the compact set; Let $\bold{x}^{i}$ the n-dimensional observations, i.e $\bold{x}^{i}= (x_{1}^{i}, \ldots, x_{n}^{i})$. Then, the normal equations for the $\omega_k \text{ (the componentes of the vector $\bold{\omega}$)}, \forall k \in \left[n\right]$ and 
the bias parameter $b$ are:\\

\begin{proof}
    \begin{equation}
        \left\{
        \begin{aligned}
                \displaystyle\sum_{\tiny 0 \leq j\leq n } \omega_j &\sum_{\tiny 0 \leq i\leq n } x^{i}_{j} x^{i}_{k}  +  b \sum_{\tiny 0 \leq i\leq n }  x^{i}_k = \displaystyle\sum_{\tiny 0 \leq i\leq n }\phi(\bold{x}^{i}) x^{i}_{k}, \forall k \in \left[n\right] \\
                \displaystyle\sum_{\tiny 0 \leq j \leq n }\omega_j &\sum_{\tiny 0 \leq i \leq n }x^{i}_{j} + nb = \displaystyle\sum_{\tiny 0 \leq i\leq n }\phi(\bold{x}^{i})
        \end{aligned}
        \right.
\end{equation}
\\
This system of equations has the following matricial expression:\\
\begin{equation}
        \begin{bmatrix}
            \displaystyle\sum_{\tiny 0 \leq i\leq n } x^{i}_{1} x^{i}_{1} & \displaystyle\sum_{\tiny0 \leq i\leq n } x^{i}_{2} x^{i}_{1} & \cdots & \displaystyle\sum_{\tiny 0 \leq i\leq n } x^{i}_{m} x^{i}_{1} & \cdots &\displaystyle\sum_{\tiny 0 \leq i\leq n }  x^{i}_1\\[1.2em]
            \displaystyle\sum_{\tiny 0 \leq i\leq n } x^{i}_{1} x^{i}_{2} & \displaystyle\sum_{\tiny 0 \leq i\leq n } x^{i}_{2} x^{i}_{2}  & \cdots & \displaystyle\sum_{\tiny 0 \leq i\leq n } x^{i}_{m} x^{i}_{2} & \cdots &\displaystyle\sum_{\tiny 0 \leq i\leq n }  x^{i}_2\\
           \vdots  & \vdots  & \ & \vdots & \ & \vdots \\
           \displaystyle\sum_{\tiny 0 \leq i\leq n } x^{i}_{1} x^{i}_{k}  & \displaystyle\sum_{\tiny 0 \leq i\leq n } x^{i}_{2} x^{i}_{k}  & \cdots & \displaystyle\sum_{\tiny 0 \leq i\leq n } x^{i}_{m} x^{i}_{k} & \cdots &\displaystyle\sum_{\tiny 0 \leq i\leq n }  x^{i}_k\\
           \vdots  & \vdots  & \ & \vdots & \ & \vdots \\
           \displaystyle\sum_{\tiny 0 \leq i\leq n } x^{i}_{1}  & \displaystyle\sum_{\tiny 0 \leq i\leq n } x^{i}_{2} & \cdots & \displaystyle\sum_{\tiny 0 \leq i\leq n } x^{i}_{m}& \cdots & n\
        \end{bmatrix}
        \begin{bmatrix}
            \omega_1 \\[1em]
            \omega_2\\
            \vdots \\
            \omega_m\\
           \vdots \\
            b
        \end{bmatrix} 
        = \begin{bmatrix}
            \displaystyle\sum_{\tiny 0 \leq i\leq n }\phi(\bold{x}^{i}) x^{i}_{1} \\[1.2em]
            \displaystyle\sum_{\tiny 0 \leq i\leq n }\phi(\bold{x}^{i}) x^{i}_{2}\\
            \vdots \\
            \displaystyle\sum_{\tiny 0 \leq i\leq n }\phi(\bold{x}^{i}) x^{i}_{m}\\
           \vdots \\
           \displaystyle\sum_{\tiny 0 \leq i\leq n }\phi(\bold{x}^{i})
        \end{bmatrix} 
\end{equation}
\\
Note that the entries of the matrix in equation $2$, are exactly the partial derivatives $\partial^2_{\omega_k\omega_k} C(\bold{\omega},b),\partial^2_{bb}C(\bold{\omega},b),\\
\partial^2_{\omega_k \omega_j} C(\bold{\omega},b),\partial^2_{\omega_k b} C(\bold{\omega},b)$ i.e such matrix is the hessian-matrix $\mathcal{H}_{C(\tiny \omega, b)}$; Let $\displaystyle\{ v_m \}_{\tiny m \in \left[ n + 1 \right]}$ be a collection of vectors in $\mathbb{R}^n$ defined as follows: $ \forall m, 0 \leq m \leq n, v_m := \displaystyle(x^{1}_{m}, \ldots ,x^{n}_{m}) \text{. For } m = n+1 \text{ we define: }v_{n+1} := \bold{1} = (1, \ldots ,1)$, and a vector $\bold{\varphi} := (\phi(\bold{x}^{1}), \ldots , \phi(\bold{x}^{n}))$. The system of equations 
can be writen as:\\
\begin{equation}
    \begin{bmatrix}
        v_1 \cdot v_1 & v_1 \cdot v_2 & \cdots & v_1 \cdot v_m & \cdots &v_1 \cdot v_{n+1}\\
        v_1 \cdot v_2 & v_2 \cdot v_2 & \cdots & v_2 \cdot v_m & \cdots &v_2 \cdot v_{n+1}\\
       \vdots  & \vdots  & \ & \vdots & \ & \vdots \\
       v_m \cdot v_1 & v_m \cdot v_2 & \cdots & v_m \cdot v_m & \cdots &v_m \cdot v_{n+1}\\
       \vdots  & \vdots  & \ & \vdots & \ & \vdots \\
       v_{n+1}\cdot v_1 & v_{n+1} \cdot v_2 & \cdots & v_{n+1} \cdot v_m & \cdots & v_{n+1} \cdot v_{n+1}\
    \end{bmatrix}
    \begin{bmatrix}
        \omega_1 \\
        \omega_2\\
        \vdots \\
        \omega_m\\
       \vdots \\
        b
    \end{bmatrix} 
    = \begin{bmatrix}
        \bold{\varphi} \cdot v_1\\
        \bold{\varphi} \cdot v_2\\
        \vdots \\
        \bold{\varphi} \cdot v_m\\
       \vdots \\
       \bold{\varphi} \cdot v_{n+1}
    \end{bmatrix} 
\end{equation}
This is  $\mathcal{H}_{C(\tiny \omega, b)} = \text{G}(v_1, \ldots v_{n+1})$ i.e the matrix in system $3$ is a Gramm matrix, which by the \href{https://en.wikipedia.org/wiki/Gram_matrix}{postively semidefiniteness property} of the Gramm matrices implies this matrix is postively defined. Then, the solution $(\bold{\omega}^{\star}, b^{\star})$ to such system is unique and this solution gives indeed a minimum for $C(\bold{\omega},b)$. Furthermore, the values of the pair $(\bold{\omega}^{\star}, b^{\star})$ are explicitly computable because 
the system is linear. 
\end{proof}
\subsection*{3.15.9}
Note that the Reyni entropy can be expressed as follows: \ $\text{H}_\alpha(\text{p}(X)) = \frac{\ln \mathbb{E} \left[(p(X))^{\alpha-1}\right]}{1 - \alpha} = \frac{\ln\displaystyle\int_{\tiny \text{sup}(X)}(p(x))^{\alpha-1} d P}{\alpha - 1}$. In the last expression $\frac{d P}{d x} = p(x)$; This representation is a consecuence of the \href{https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem}{Radon-Nikodym's Theorem}. Now, let's analyse the following function definied as a parametric integral. Let $I(\alpha)$ definied as:\\
$I(\alpha) = \displaystyle\int_{\tiny \text{sup}(X)}(p(x))^{\alpha-1} d P$\newline 
\begin{proof}
    \begin{itemize}
        \item \underline{CASE $0 < p(x) < 1$}:\\
        \\
        Let $\{\alpha_n\}_{n \in \mathbb{N}} \subset \mathbb{R}$ a monotonicaly decreasing sequence of reals tending to 1. With this we can construct an increasing sequence of P-integrable functions of the form $\{(p(x))^{\alpha_n -1}\}_{n \in \mathbb{N}}$. Then, by 
        construction the point limit will be the function $f(x) = 1$ which is P-integrable. Applying the \href{https://proofwiki.org/wiki/Monotone_Convergence_Theorem_(Measure_Theory)}{Lebegue's monotone convergence theorem}:\\
        $\lim_{k \to \infty} I(\alpha_k) = \displaystyle\lim_{k \to \infty} \int_{\tiny \text{sup}(X)}(p(x))^{\alpha_k-1} d P = \displaystyle\int_{\tiny \text{sup}(X)} \lim_{k \to \infty}(p(x))^{\alpha_k-1} d P  = I(\lim_{k \to \infty} \alpha_k) = \lim_{\alpha \to 1^{+}}I(\alpha) = 1$. This proves $I(\alpha)$ is right-continuous\newline
        \\
        Taking $\{\alpha_n\}_{n \in \mathbb{N}} \subset \mathbb{R}$ a monotonicaly increasing sequence of reals tending to 1, let's construct the sequence of decreasing P-integrable functions of the form $\{(p(x))^{\alpha_n -1}\}_{n \in \mathbb{N}}$. Once again the point limit function is $f(x) = 1$. Applying the monotone convergence \\
        $\lim_{k \to \infty} I(\alpha_k) = \displaystyle\lim_{k \to \infty} \int_{\tiny \text{sup}(X)}(p(x))^{\alpha_k-1} d P = \displaystyle\int_{\tiny \text{sup}(X)} \lim_{k \to \infty}(p(x))^{\alpha_k-1} d P  = I(\lim_{k \to \infty} \alpha_k) = \lim_{\alpha \to 1^{-}}I(\alpha) = 1$. This proves $I(\alpha)$ is left-continuous. \\\
        \item  \underline{CASE $p(x) > 1$}:\\
        \\
        The same is true in this case. It is worth to note that if the sequence $\{\alpha_n\}_{n \in \mathbb{N}}$ is choosen to be decreasing then the sequence of functions is decreasing, the contrary situation also holds.\\
    \end{itemize}    
    Because the function $f(\alpha,x) = (p(x))^{\alpha - 1}$ is derivable, and its derivative is continuous (this can be proved by the monotone convergence theorem), the above can be then used to compute the limit $\lim_{\alpha \to 1 }\text{H}_\alpha(\text{p}(X))$. By a simple
    application $\alpha \to 1$ we find an indetermination of the type $0/0$; Using L'Hôpital's rule we get:\\
    $\displaystyle\lim_{\alpha \to 1 }\text{H}_\alpha(\text{p}(X)) =\displaystyle\lim_{\alpha \to 1}\frac{1}{-1}\frac{1}{\displaystyle\int_{\tiny \text{sup}(X)}(p(x))^{\alpha-1} d P}\int_{\tiny \text{sup}(X)}(p(x))^{\alpha-1} \ln(p(x)) d P = \displaystyle \int_{\tiny \text{sup}(X)}-\ln(p(x)) d P = \text{H}(p).$
\end{proof}

\subsection*{3.15.10 (a)}
This is a consecuence of exercise $3.15.9.b$. By taking $\sigma_1 = \sigma_2 = \sigma$ we get that $\varphi_{\sigma_1}(x) \ \star \ \varphi_{\sigma_2}(x) = \varphi_{\sigma^\prime}(x)$, with $\sigma^\prime = \displaystyle\sqrt{2\sigma^2}$
\subsection*{3.15.10 (b)}
We have $\phi_{\sigma}(x) := \displaystyle\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{\frac{-x^2}{2\sigma^2}}$; Computing $(\phi_{\sigma_1}\star \phi_{\sigma_2})(x) = \displaystyle \int_{\mathbb{R}}  \phi_{\sigma_1}(t) \phi_{\sigma_2}(t - x) d t $ we get:\\
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            (\phi_{\sigma_1}\star \phi_{\sigma_2})(x) &= \displaystyle \int_{\mathbb{R}}\displaystyle\frac{1}{\sqrt{2\pi\sigma^{2}_1\sigma^{2}_2}\sqrt{2\pi}} e^{\displaystyle\frac{-t^2\sigma^{2}_2 -(t - x)\sigma^{2}_1}{2\sigma_1^2 \sigma_2^2}} d t = \displaystyle \int_{\mathbb{R}} \displaystyle\frac{1}{\sqrt{2\pi\sigma^{2}_1\sigma^{2}_2}\sqrt{2\pi}} e^{-\displaystyle\frac{t^{2}(\sigma^{2}_1 + \sigma^{2}_2) + 2tx\sigma^{2}_1 - x^{2}\sigma^{2}_1}{2\sigma^{2}_1\sigma^{2}_2}} d t \\
            &= \displaystyle \int_{\mathbb{R}} \displaystyle\frac{\sqrt{\sigma_1^2 + \sigma^{2}_2}}{\sqrt{2\pi\sigma^{2}_1\sigma^{2}_2}\sqrt{2\pi}\sqrt{\sigma_1^2 + \sigma^{2}_2}} e^{-\displaystyle(\sigma^{2}_1 + \sigma^{2}_2)\frac{t^{2} - 2tx\frac{\sigma^{2}_1}{\sigma^{2}_1 + \sigma^{2}_2} + \frac{x^{2}(\sigma^{2}_1)^{2}}{(\sigma^{2}_1 + \sigma^{2}_2)^{2}} + x^{2}\sigma^{2}_1 - \frac{x^{2}(\sigma^{2}_1)^{2}}{(\sigma^{2}_1 + \sigma^{2}_2)^{2}} }{2\sigma^{2}_1\sigma^{2}_2}} d t \\
            &= \displaystyle \int_{\mathbb{R}} \displaystyle\frac{1}{\sqrt{2\pi(\sigma_1^2 + \sigma^{2}_2)}} e^{-\displaystyle\frac{x^{2}}{2(\sigma^{2}_1 + \sigma^{2}_2)}}\displaystyle\frac{1}{\sqrt{2\pi \frac{\sigma^{2}_1\sigma^{2}_2}{\sigma^{2}_1 + \sigma^{2}_2}}} e^{-\displaystyle\frac{(t - x\frac{\sigma^{2}_1}{\sigma^{2}_1 + \sigma^{2}_2})^{2}}{2 \frac{\sigma^{2}_1\sigma^{2}_2}{\sigma^{2}_1 + \sigma^{2}_2}}} d t \\
            &= \displaystyle\frac{1}{\sqrt{2\pi(\sigma_1^2 + \sigma^{2}_2)}} e^{-\displaystyle\frac{x^{2}}{2(\sigma^{2}_1 + \sigma^{2}_2)}} \displaystyle \int_{\mathbb{R}} \displaystyle\frac{1}{\sqrt{2\pi \frac{\sigma^{2}_1\sigma^{2}_2}{\sigma^{2}_1 + \sigma^{2}_2}}} e^{-\displaystyle\frac{(t - x\frac{\sigma^{2}_1}{\sigma^{2}_1 + \sigma^{2}_2})^{2}}{2 \frac{\sigma^{2}_1\sigma^{2}_2}{\sigma^{2}_1 + \sigma^{2}_2}}} d t \\
            &=\displaystyle \frac{1}{\sqrt{2\pi(\sigma^{2}_1 + \sigma^{2}_2)}}e^{\displaystyle\frac{-x^2}{2(\sigma_1^2 + \sigma^{2}_2)}} \cdot 1 = \phi_{\sigma^{\prime}}(x) \text{ , with } (\sigma^{\prime})^{2} = \sigma_1^2 + \sigma^{2}_2\\
        \end{aligned}
    \end{equation*}
\end{proof}

\subsection*{3.15.11 (a)}
\begin{proof}  
    \begin{itemize}
        \item \underline{CASE $p=q$}:\newline
        \\
        Making the substitution in the formula for the Cauchy-Schwartz- divergence, we find:\\
        $D_{CS}(p,p) = -\ln(\displaystyle\frac{\int p(x) p(x) d x}{\sqrt{\int p(x)^2 d x } \sqrt{\int p(x)^2 d x }}) = - \ln(1) = 0.$
        \item \underline{CASE $D_{CS}(p,q) = 0$}:\\
        \\
        If $D_{CS}(p,q) = 0$, then applying exponentials to both sides we obtain:\newline
        $\displaystyle\frac{\displaystyle \int p(x) q(x) d x}{\sqrt{\displaystyle\int p(x)^2 d x } \sqrt{\displaystyle\int q(x)^2 d x }} = 1 \implies \lVert \displaystyle\int p(x) q(x) d x \lVert = \sqrt{\displaystyle \int p(x)^2 d x } \sqrt{\displaystyle \int q(x)^2 d x }$. This is the case of equality in
        the Cauchy–Bunyakovsky–Schwarz inequality. Then: $q = \lambda p, \lambda \neq 0$; Let's now prove $\lambda = 1$.
        \\
        Plugging this we get: \\
        $\displaystyle\frac{\displaystyle \lambda \int p(x) p(x) d x}{\lambda^{2} \sqrt{\displaystyle\int p(x)^2 d x } \sqrt{\displaystyle\int p(x)^2 d x }} = 1 = \frac{\lambda}{\lambda^{2}} \implies \lambda  = 1 \implies p = q.$
    \end{itemize}  
\end{proof}

\subsection*{3.15.11 (b)}
By hypothesis both $p, q$ are densities, then they are both nonnegative. This implies: \\

\begin{proof} 
    $\displaystyle \int p(x) q(x) d x  = \displaystyle \int \left| \ p(x) q(x) \ \right| d x \leq  {\lVert p \lVert}^{2}_{2} {\lVert q \lVert}^{2}_{2} \implies 0 < \displaystyle\frac{\displaystyle \int p(x) q(x) d x}{{\lVert p \lVert}^{2}_{2} {\lVert q \lVert}^{2}_{2}} \leq 1$; Remember that $\ln(x) \geq 0 $ if $0<x<1$ then, taking logarithms to both sides in the inequality and
    multiplying by $-1$ yields:\\
    $D_{CS}(p,q) = -\ln(\displaystyle \frac{\displaystyle \int p(x) q(x) d x }{\displaystyle\sqrt{\int p(x)^2 d x } \displaystyle\sqrt{\int q(x)^2 d x }}) \geq 0$
\end{proof} 

\subsection*{3.15.11 (c)}
This propiety follows easily from the definition of the CS-divergence. 

\subsection*{3.15.11 (d)}
Applying the properties of the logarithms we obtain:\\
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            D_{CS}(p,q) &= -\ln(\displaystyle \frac{\displaystyle \int p(x) q(x) d x }{\displaystyle\sqrt{\int p(x)^2 d x } \displaystyle\sqrt{\int q(x)^2 d x }}) = -\ln \displaystyle \int p(x) q(x) d x - \frac{1}{2}\ln \displaystyle\int p(x)^2 d x - \frac{1}{2} \displaystyle\int q(x)^2 d x\\
                        &=  -\ln \displaystyle \int p(x) q(x) d x - \frac{1}{2}\text{H}_2(p) - \frac{1}{2}\text{H}_2(q)
        \end{aligned}    
    \end{equation*}    
\end{proof}    
\end{document}

