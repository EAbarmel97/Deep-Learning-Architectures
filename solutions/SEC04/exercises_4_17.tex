\documentclass{exam}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{ragged2e}
\usepackage{lmodern}
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{verbatim}

\pagestyle{empty}
\renewcommand{\theenumi}{\alph{enumi}}
\renewenvironment{proof}{{\noindent\itshape\ignorespaces}}{{\hfill$\qed$\\}}

\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\begin{center}
    \textbf{\Large Exercises 4.17 }
\end{center}

\section*{Exercise 4.17.1}
Let $f(x_1,x_2) = e^{x_1}\sin(x_2)$, with $(x_1,x_2) \in (0,1) \times (0,\frac{\pi}{2})$.
\begin{enumerate}
    \item Show that $f$ is a harmonic function;
    \item Find $\lVert \nabla f \lVert$;
    \item Show that the equation $\nabla f = 0$ does not have any solutions;
    \item Find the maxima and minima for the function $f$.
\end{enumerate}

\section*{Exercise 4.17.2}
Consider the quadratic function $Q(\bold{x}) = \frac{1}{2} \bold{x}^{T} A \bold{x} - b \bold{x}$, with A nonsingular square matrix of order $n$.
\begin{enumerate}
    \item Find the gradient $\lVert \nabla Q \lVert$;
    \item Write the gradient descent iteration;
    \item Find the Hessian $H_{Q}$;
    \item Write the iteration by Newton's formula and compute its limit.
\end{enumerate}

\section*{Exercise 4.17.3}
Let A be a nonsingular square matrix of order $n$ and $b \in \mathbb{R}^{n}$ a given vector. Consider the linear system $A\bold{x} = b$. The solution can be approximated using 
the following steps:\\
\begin{enumerate}
    \item Associate the cost function $C(\bold{x}) = \frac{1}{2} \lVert A \bold{x} - b \lVert^{2}.$ Find its gradient, $\nabla C(\bold{x})$,
    and Hessian $H_{C}(\bold{x})$;
    \item Write the gradient descent algorithm iteration which converges to the system solution $\bold{x}$ with the inital value $\bold{x}^{0} = 0$;
    \item Write Newton's iteration which converges to the system solution $\bold{x}$ with the initial value $\bold{x}^{0} = 0$.
\end{enumerate}

\section*{Exercise 4.17.4}
\begin{enumerate}
   \item Let $(a_n)_{n}$ be a sequence with $a_0 > 0$ satisfying the inequality \\
   $a_{n+1} \leq \mu a_n + K, \ \forall n \geq 1$, with $\mu \in (0,1)$ and $K > 0$. Show that the sequence $(a_n)_{n}$ is bounded from above.
   \item Consider the momentum method equations $(4.4.16)-(4.4.17)$, and asume that the function $f$ has a bounded gradient $\lVert \nabla f \lVert \leq M$. Show that the 
   sequence of velocities, $(v^n)_{n}$ is bounded.
\end{enumerate}

\section*{Exercise 4.17.5}
\begin{enumerate}
   \item  Let $f$ and $g$ two integrable functions. Verify that
            \begin{equation*}
                \int (f \star g) (x) d x  = \displaystyle \int f(x) d x \displaystyle \int g(x) d x 
            \end{equation*}

    \item Show that $\lVert f \star g \lVert \leq {\lVert f \lVert}_{1} {\lVert g \lVert}_{1}$
    \item Let $f_{\sigma} := g \star G_{\sigma}$ where $G_{\sigma} = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^{2}}{2\sigma^{2}}}$. Prove that ${\lVert f_{\sigma}\lVert}_{1} \leq {\lVert f \lVert}_{1}$ for $\sigma > 0$
\end{enumerate}

\section*{Exercise 4.17.6}
Show that the convolution of two Gaussians is also a Gaussian:\\
\begin{equation*}
    G_{\sigma_{1}} \star G_{\sigma_{2}} = G_{\sigma} \text{, where } \sigma = \sqrt{\sigma_1^{2} + \sigma_2^{2}}.
\end{equation*}

\section*{Exercise 4.17.7}
Show that the if $n$ have the sum equal to s,
\begin{equation*}
    \sigma_{1} + \ldots + \sigma_{n} = s,
\end{equation*}
then the numbers for which the sum of their squares, $\sum_{j=1}^{n} \sigma_{j}^{2}$, its minimum occurs for the case when all the numbers are equal to $\frac{s}{n}.$

\newpage

\begin{center}    
    \section*{SOLUTIONS}
\end{center}

\subsection*{Exercise 4.17.1 (a)}
    By definition a function is harmonic when satisffies the condition $\nabla^{2}f = 0$. Let's corroborate this is indeed fullfilied by the function 
    $f(x_1,x_2) = e^{x_1}\sin(x_2)$. For $\frac{\partial^{2}}{ \partial x^{2}_1}f$ and $\frac{\partial^{2}}{ \partial x^{2}_2}f$ we have:\\
    \begin{proof}
        \begin{equation*}     
            \begin{aligned}
                \frac{\partial^{2}}{ \partial x^{2}_1} e^{x_1}\sin(x_2) &=  \sin(x_2)\frac{\partial^{2}}{ \partial x^{2}_1} e^{x_1} = e^{x_1}\sin(x_2)\\
                \frac{\partial^{2}}{ \partial x^{2}_2} e^{x_1}\sin(x_2) &=  e^{x_1}\frac{\partial^{2}}{ \partial x^{2}_2}\sin(x_2)  = -e^{x_1}\sin(x_2)\\
            \end{aligned}
    \end{equation*}
    From the latter follows $\nabla^{2}f = \displaystyle\frac{\partial^{2}}{ \partial x^{2}_1}f + \displaystyle\frac{\partial^{2}}{ \partial x^{2}_2}f = 0$ i.e the function $f$ is harmonic.
    \end{proof}
\subsection*{Exercise 4.17.1 (b)}
By the pythagorean identity between the trigonometric functions $\sin$ and $\cos$ follows:\\
\begin{equation*}
    \lVert \nabla f \lVert = \sqrt{\nabla f \cdot \nabla f} = \sqrt{(e^{x_1}\cos(x_2))^{2} + (e^{x_1}\sin{x_2})^{2}} = e^{x_1}
\end{equation*}
\subsection*{Exercise 4.17.1 (c)}
\begin{proof}
    \begin{equation*}     
        \begin{aligned}
            \nabla f = (e^{x_1}\cos(x_2), e^{x_1}\sin(x_2)) = 0 \iff e^{x_1} = 0 
        \end{aligned}
\end{equation*}
The equation $e^{x} = 0$ is known to not have a solution. Therefore, $\nabla f = 0$ is not solvable.
\end{proof}
\subsection*{Exercise 4.17.1 (d)}
\begin{proof}
    Let's define the extension of $f$ over the compact $K := [0,1] \times [0, \frac{\pi}{2}]$ with the same association rule as above. On this set $f$ is also harmonic. Then, 
    f reaches its minimun and maximum  on the boundaries of the set $K$; note that both functions $e^{x_1}$ and $\sin(x_2)$ are increasing, then the maxima is met at the point when both functions
    reach their maximum. \\
    
    \noindent This means the maximum of the function $f$ is met on the point $(1,\frac{\pi}{2})$. Likewise, the minimum is reached at the point $(0,0)$.
\end{proof}

\subsection*{Exercise 4.17.2 (a)}
Computing the gradient we get: $\nabla Q = \frac{1}{2}(\bold{A}  + \bold{A}^{T})\bold{x} + b\boldsymbol{1}$.

\subsection*{Exercise 4.17.2 (b)}
The equations that describe the iterations made in the GDA is the sequence of vectors ${\bold{x}^n}_{n \in \mathbb{N}}$ sattisfying the following 
recursion:
$\bold{x}^{n+1} = \bold{x}^{n} - \eta \displaystyle\frac{\frac{1}{2}(\bold{A}  + \bold{A}^{T})\bold{x}^{n} + b\boldsymbol{1}}{\lVert \frac{1}{2}(\bold{A}  + \bold{A}^{T})\bold{x}^{n} + b\boldsymbol{1} \lVert}$ 
\subsection*{Exercise 4.17.2 (c)}
From excercise $4.17.2.a$ we know that $\nabla Q = \frac{1}{2}(\bold{A}  + \bold{A}^{T})\bold{x} + b\boldsymbol{1}$. Note that taking the derivative of a vectorial 
function is indeed the same as computing its Hessian-matrix; By using the afore mentioned observation, we get:\\

$H_{Q}(\bold{x}) = \frac{\partial}{\partial \bold{x}} \nabla Q = \frac{1}{2}(\bold{A}  + \bold{A}^{T})$.

\subsection*{Exercise 4.17.2 (d)}
The sequence of iterations $\{\bold{x}^n\}_{n \in \mathbb{N}}$ using the Newton method areg given with following recurrence 
relationship:\\
\begin{equation*}
    \begin{aligned}
        \bold{x}^{n+1} &= \bold{x}^{n} - \displaystyle (\frac{1}{2}(\bold{A}  + \bold{A}^{T}))^{-1}(\frac{1}{2}(\bold{A}  + \bold{A}^{T})\bold{x}^{n} - b\boldsymbol{1})\\
        &= \displaystyle (\frac{1}{2}(\bold{A}  + \bold{A}^{T}))^{-1} b\boldsymbol{1}.
    \end{aligned}
\end{equation*}
\end{document}