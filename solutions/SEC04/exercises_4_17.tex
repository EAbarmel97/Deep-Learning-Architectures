\documentclass{exam}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{ragged2e}
\usepackage{lmodern}
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage[bottom]{footmisc}

\pagestyle{empty}
\renewcommand{\theenumi}{\alph{enumi}}
\renewenvironment{proof}{{\noindent\itshape\ignorespaces}}{{\hfill$\qed$\\}}

\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\begin{center}
    \textbf{\Large Exercises 4.17 }
\end{center}

\section*{Exercise 4.17.1}
Let $f(x_1,x_2) = e^{x_1}\sin(x_2)$, with $(x_1,x_2) \in (0,1) \times (0,\frac{\pi}{2})$.
\begin{enumerate}
    \item Show that $f$ is a harmonic function;
    \item Find $\lVert \nabla f \lVert$;
    \item Show that the equation $\nabla f = 0$ does not have any solutions;
    \item Find the maxima and minima for the function $f$.
\end{enumerate}

\section*{Exercise 4.17.2}
Consider the quadratic function $Q(\bold{x}) = \frac{1}{2} \bold{x}^{T} A \bold{x} - b \bold{x}$, with A nonsingular square matrix of order $n$.
\begin{enumerate}
    \item Find the gradient $\lVert \nabla Q \lVert$;
    \item Write the gradient descent iteration;
    \item Find the Hessian $H_{Q}$;
    \item Write the iteration by Newton's formula and compute its limit.
\end{enumerate}

\section*{Exercise 4.17.3}
Let A be a nonsingular square matrix of order $n$ and $b \in \mathbb{R}^{n}$ a given vector. Consider the linear system $A\bold{x} = b$. The solution can be approximated using 
the following steps:\\
\begin{enumerate}
    \item Associate the cost function $C(\bold{x}) = \frac{1}{2} \lVert A \bold{x} - \bold{b} \lVert^{2}.$ Find its gradient, $\nabla C(\bold{x})$,
    and Hessian $H_{C}(\bold{x})$;
    \item Write the gradient descent algorithm iteration which converges to the system solution $\bold{x}$ with the inital value $\bold{x}^{0} = 0$;
    \item Write Newton's iteration which converges to the system solution $\bold{x}$ with the initial value $\bold{x}^{0} = 0$.
\end{enumerate}

\section*{Exercise 4.17.4}
\begin{enumerate}
   \item Let $(a_n)_{n}$ be a sequence with $a_0 > 0$ satisfying the inequality \\
   $a_{n+1} \leq \mu a_n + K, \ \forall n \geq 1$, with $\mu \in (0,1)$ and $K > 0$. Show that the sequence $(a_n)_{n}$ is bounded from above.
   \item Consider the momentum method equations $(4.4.16)-(4.4.17)$, and asume that the function $f$ has a bounded gradient $\lVert \nabla f \lVert \leq M$. Show that the 
   sequence of velocities, $(v^n)_{n}$ is bounded.
\end{enumerate}

\section*{Exercise 4.17.5}
\begin{enumerate}
   \item  Let $f$ and $g$ two integrable functions. Verify that
            \begin{equation*}
                \int (f \star g) (x) d x  = \displaystyle \int f(x) d x \displaystyle \int g(x) d x 
            \end{equation*}

    \item Show that $\lVert f \star g \lVert \leq {\lVert f \lVert}_{1} {\lVert g \lVert}_{1}$
    \item Let $f_{\sigma} := f \star G_{\sigma}$ where $G_{\sigma} = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^{2}}{2\sigma^{2}}}$. Prove that ${\lVert f_{\sigma}\lVert}_{1} \leq {\lVert f \lVert}_{1}$ for $\sigma > 0$
\end{enumerate}

\section*{Exercise 4.17.6}
Show that the convolution of two Gaussians is also a Gaussian:\\
\begin{equation*}
    G_{\sigma_{1}} \star G_{\sigma_{2}} = G_{\sigma} \text{, where } \sigma = \sqrt{\sigma_1^{2} + \sigma_2^{2}}.
\end{equation*}

\section*{Exercise 4.17.7}
Show that the if $n$ have the sum equal to s,
\begin{equation*}
    \sigma_{1} + \ldots + \sigma_{n} = s,
\end{equation*}
then the numbers for which the sum of their squares, $\sum_{j=1}^{n} \sigma_{j}^{2}$, its minimum occurs for the case when all the numbers are equal to $\frac{s}{n}.$

\newpage

\begin{center}    
    \section*{SOLUTIONS}
\end{center}

\subsection*{Exercise 4.17.1 (a)}
    By definition a function is harmonic when satisfies the condition $\nabla^{2}f = 0$. Let's corroborate this is indeed fullfilied by the function 
    $f(x_1,x_2) = e^{x_1}\sin(x_2)$. For $\frac{\partial^{2}}{ \partial x^{2}_1}f$ and $\frac{\partial^{2}}{ \partial x^{2}_2}f$ we have:\\
    \begin{proof}
        \begin{equation*}     
            \begin{aligned}
                \frac{\partial^{2}}{ \partial x^{2}_1} e^{x_1}\sin(x_2) &=  \sin(x_2)\frac{\partial^{2}}{ \partial x^{2}_1} e^{x_1} = e^{x_1}\sin(x_2)\\
                \frac{\partial^{2}}{ \partial x^{2}_2} e^{x_1}\sin(x_2) &=  e^{x_1}\frac{\partial^{2}}{ \partial x^{2}_2}\sin(x_2)  = -e^{x_1}\sin(x_2)\\
            \end{aligned}
    \end{equation*}
    From the latter follows $\nabla^{2}f = \displaystyle\frac{\partial^{2}}{ \partial x^{2}_1}f + \displaystyle\frac{\partial^{2}}{ \partial x^{2}_2}f = 0$ i.e the function $f$ is harmonic.
    \end{proof}
\subsection*{Exercise 4.17.1 (b)}
By the pythagorean identity between the trigonometric functions $\sin$ and $\cos$ follows:\\
\begin{equation*}
    \lVert \nabla f \lVert = \sqrt{\nabla f \cdot \nabla f} = \sqrt{(e^{x_1}\cos(x_2))^{2} + (e^{x_1}\sin{x_2})^{2}} = e^{x_1}
\end{equation*}
\subsection*{Exercise 4.17.1 (c)}
\begin{proof}
    \begin{equation*}     
        \begin{aligned}
            \nabla f = (e^{x_1}\cos(x_2), e^{x_1}\sin(x_2)) = 0 \iff e^{x_1} = 0 
        \end{aligned}
\end{equation*}
The equation $e^{x} = 0$ is known to not have a solution. Therefore, $\nabla f = 0$ is not solvable.
\end{proof}
\subsection*{Exercise 4.17.1 (d)}
\begin{proof}
    Let's define the extension of $f$ over the compact $K := [0,1] \times [0, \frac{\pi}{2}]$ with the same association rule as above. On this set $f$ is also harmonic. Then, 
    f reaches its minimun and maximum  on the boundaries of the set $K$; note that both functions $e^{x_1}$ and $\sin(x_2)$ are increasing, then the maxima is met at the point when both functions
    reach their maximum. \\
    
    \noindent This means the maximum of the function $f$ is met on the point $(1,\frac{\pi}{2})$ with a value of $f(1,\pi) = e$. Likewise, the minimum is reached at the point $(0,0)$ with a value of $f(0,0) = 0$.
\end{proof}

\subsection*{Exercise 4.17.2 (a)}
Computing the gradient we get: $\nabla Q = \frac{1}{2}(A  + {A}^{T})\bold{x} - b\boldsymbol{1}$. Then, expressing the norm in terms of the interior product we have:\\
$\lVert \nabla Q(\bold{x}) \lVert = \displaystyle (\frac{1}{4}\bold{x}^{T}(A + {A}^{T})^{2}\bold{x} - b\boldsymbol{1}(A + {A}^{T})\bold{x} + b^{2}{\boldsymbol{1}}^{T}\boldsymbol{1})^{\frac{1}{2}}$

\subsection*{Exercise 4.17.2 (b)}
The equations that describe the iterations made in the GDA\footnotemark[1]\footnotetext[1]{From now on, the learning rate $\delta$ in the GDA an its variants will be supposed constant, unless is stated otherwise. } is the sequence of vectors $\{\bold{x}^n\}_{n \in \mathbb{N}}$ satisfying the following 
recursion:
$\bold{x}^{n+1} = \bold{x}^{n} - \delta \displaystyle(\frac{1}{2}(A  + {A}^{T})\bold{x}^{n} - b\boldsymbol{1})$ 


\subsection*{Exercise 4.17.2 (c)}
From excercise $4.17.2.a$ we know that $\nabla Q = \frac{1}{2}(A  + {A}^{T})\bold{x} + b\boldsymbol{1}$. Note that taking the derivative of a vectorial 
function is indeed the same as computing its Hessian-matrix. By using the afore mentioned observation, we get:\\

$H_{Q}(\bold{x}) = \frac{\partial}{\partial \bold{x}} \nabla Q = \frac{1}{2}(A  + {A}^{T})$.

\subsection*{Exercise 4.17.2 (d)}
The sequence of iterations produced by Newton's method $\{\bold{x}^n\}_{n \in \mathbb{N}}$ are given with following recurrence 
relationship:\\
\begin{equation*}
    \begin{aligned}
        \bold{x}^{n+1} &= \bold{x}^{n} - \displaystyle (\frac{1}{2}(A  + {A}^{T}))^{-1}(\frac{1}{2}(A  + {A}^{T})\bold{x}^{n} - b\boldsymbol{1})\\
        &= \displaystyle (\frac{1}{2}(A  + {A}^{T}))^{-1} b\boldsymbol{1}.
    \end{aligned}
\end{equation*}
We note that the sequence is a constant. This in turn implies the limit is trivialy given by the expression: $\bold{x}^{\star} = \displaystyle (\frac{1}{2}(A  + {A}^{T}))^{-1} b\boldsymbol{1}$.

\subsection*{Exercise 4.17.3 (a)}
Expressing $C$ as a quadratic form and simplifying we get: 
$C(\bold{x}) = \frac{1}{2}\bold{x}^{T} {A}^{T} A \bold{x} - \bold{b}^{T} A \bold{x} + \frac{1}{2} \bold{b}^{T} \bold{b}$. On the other hand, the fact that the matrix $A^{T}A$ is symmetric, yields:
\begin{itemize}
    \item $\nabla C(\bold{x})$ : 
    \begin{equation*}
        \hspace{-6cm}
        \begin{aligned}
            \frac{\partial}{\partial \bold{x}}C(\bold{x})&= \frac{\partial}{\partial \bold{x}}\left[\frac{1}{2}\bold{x}^{T} {A}^{T} A \bold{x} - \bold{b}^{T} A \bold{x} + \frac{1}{2} \bold{b}^{T} \bold{b}\right] \\  
            &= {A}^{T} A \bold{x} - \bold{b}^{T} A 
        \end{aligned}    
    \end{equation*}

    \item $H_{C}(\bold{x})$ : 
    \begin{equation*}
        \hspace{-8cm}
        \begin{aligned}
            \frac{\partial^{2}}{\partial^{2} \bold{x}}C(\bold{x})&= \frac{\partial}{\partial \bold{x}}\left[ {A}^{T} A \bold{x} - \bold{b}^{T} A \right] \\  
            &= {A}^{T} A 
        \end{aligned}    
    \end{equation*}
\end{itemize}

\subsection*{Exercise 4.17.3 (b)}
Using exercise 4.17.3.a is clear that the iteration in the GDA method is: $\bold{x}^{n+1} = (Id - \delta A^{T}A)\bold{x}^{n} + \delta \bold{b}^{T}A$. Iterating the latter 
equations gives: \\
\begin{equation*}
    \begin{aligned}
        \bold{x}^{n+1} &= \displaystyle (Id - \delta A^{T}A)\bold{x}^{n} + \delta \bold{b}^{T}A \\
        \vdots&\\
        \bold{x}^{n+1} &= \displaystyle (Id - \delta A^{T}A)^{n}\bold{x}^{0} + \delta \sum_{k=0}^{n-1}(Id - \delta A^{T}A)^{n}\bold{b}^{T}A
    \end{aligned}    
\end{equation*}

\noindent Note that the eigenvalues of are all positive, and furthermore in absolute value less than one (The proof of this can be found in the Apendix G). Implying the sequence \\
$\displaystyle\sum_{k=0}^{n-1}(Id - \delta A^{T}A)^{n}$ converges, which in turn implies 
$\displaystyle(Id -\delta A^{T}A)^{n} \to \mathbb{O}$  and that the limit $\bold{x}^{\star}$ is well definied and has the value: $\bold{x}^{\star} = (A^{T}A)^{-1}\bold{b}^{T}A$.

\subsection*{Exercise 4.17.3 (c)}
The sequence of Newton's method iterations is given by:\\
\begin{equation*}
    \begin{aligned}
        \bold{x}^{n+1} &= \bold{x}^{n} - (A^{T}A)^{-1}(A^{T}A\bold{x}^{n} - \bold{b}^{T}A)\\
        &= \bold{x}^{n} - \bold{x}^{n} + (A^{T}A)^{-1}\bold{b}^{T}A \implies \bold{x}^{\star} = \displaystyle \lim_{n \to \infty}\bold{x}^{n} = (A^{T}A)^{-1}\bold{b}^{T}A .
    \end{aligned} 
\end{equation*}

\subsection*{Exercise 4.17.4 (a)}
The sequence $\displaystyle\{ a_n \}_{ n \in \mathbb{N}}$ satisfies the inequality $a_{n+1}  \leq \mu a_{n} + K \text{ , } \forall n \geq 1$. Then, iterating this
inequality we have:\\
\begin{proof}
    \begin{equation*}
        \hspace{-5cm}
        \begin{aligned}
            a_{n+1}  &\leq \mu a_{n} + K\\
            &\leq \mu^{2}a_{n-1} + (1 + \mu)K \leq \ldots \leq \mu^{n} a_{0} + K\sum_{j=0}^{n-1} \mu^{j}
        \end{aligned}
    \end{equation*}
\noindent By hypothesis $a_0 > 0 \text{ , } \mu \in (0,1)$ and $K > 0$. Implying the sequence of partials sums $\displaystyle\{\sum_{j=0}^{n-1} \mu^{j}\}_{\{n \ : \ n \geq 1\}}$ is convergent. Thus 
the sequence $\displaystyle\{ a_n \}_{ n \in \mathbb{N}}$ is bounded by above.
\end{proof}

\subsection*{Exercise 4.17.4 (b)}
If the function $f$ satisfies $\lVert \nabla f \lVert \leq M$. Then, the set of equations describying the Momentum GDA (4.4.16-4.4.17) in norm satisfy the following inequalities:\\
\\
\begin{proof}
    \begin{equation*}
        \hspace{-10cm}
        \begin{cases}
            \lVert \bold{x}^{n+1} \lVert \ \leq \lVert \bold{x}^{n} \lVert  +  \lVert \bold{v}^{n+1} \lVert \\[1em]
            \lVert \bold{v}^{n+1} \lVert  \ \leq \mu \lVert \bold{v}^{n} \lVert + \eta M 
        \end{cases}
    \end{equation*}\\
\noindent An application of the above exercise with $a_{n} = \lVert \bold{v}^{n} \lVert \text { , } K = \eta M$ yields the desired result.   
\end{proof}

\subsection*{Exercise 4.17.5 (a)}
\begin{proof}
    Without loss of generality let's assume that the convolution product $f \star g$ is being integrated over the whole real line. Then, by the definition of the 
    convolution product we get:\\
    \begin{equation*}
        \begin{aligned}
            \int_{\mathbb{R}} (f \star g) (x) d x  &= \displaystyle \int_{\mathbb{R}} \int_{\mathbb{R}} f(u) g(u-x) du dx = \displaystyle \int_{\mathbb{R}} \int_{\mathbb{R}} f(v) g(t) dv dt   \ \text{ (applying the substitution: } u = v \text{,}  u- x = t \text{)}\\
            &= \int_{\mathbb{R}} f(v) d v \displaystyle \int_{\mathbb{R}} g(t) dt
        \end{aligned}    
    \end{equation*}
\noindent Where the last equality follows from \href{https://en.wikipedia.org/wiki/Fubini%27s_theorem}{Fubbini's theorem}.
\end{proof}

\subsection*{Exercise 4.17.5 (b)}
\begin{proof}
    An application of the integral triangle inequality to the above identity, yields:
    \begin{equation*}
        \begin{aligned}
        \left| \int_{\mathbb{R}} (f \star g) (x) d x  \right| &\leq \int_{\mathbb{R}} \left|  (f \star g )(x) \right|d x = {\lVert (f\star g )(x) \lVert}_{1} = \int_{\mathbb{R}} \int_{\mathbb{R}} \left|  f (x) \right| \left|  g(u-x) \right| du dx \\
        &\leq \int_{\mathbb{R}} \left|  f (x) \right| d x  \int_{\mathbb{R}} \left|  g (x) \right| d x =  {\lVert f(x) \lVert}_{1}{\lVert g(x) \lVert}_{1}
        \end{aligned}    
    \end{equation*} 
\end{proof}
\subsection*{Exercise 4.17.5 (c)}
\begin{proof}
    \noindent Noting that $G_{\sigma}$ is a density, then ${\lVert G_{\sigma}(x) \lVert}_{1} = 1$. Using the inequality proven in excercise 4.17.5.b we get:\\
    \begin{equation*}
        {\lVert f_{\sigma}(x) \lVert}_{1} \leq {\lVert f(x) \lVert}_{1}{\lVert G_{\sigma}(x) \lVert}_{1} \leq {\lVert f(x) \lVert}_{1} 
    \end{equation*}
\end{proof}    

\subsection*{Exercise 4.17.6}
This is in esence what was proven in Exercise 3.15.10.b  

\subsection*{Exercise 4.17.7}
Let $\boldsymbol{\sigma} := (\sigma_1, \ldots, \sigma_n)$. One has to optimize the function $\displaystyle f(\boldsymbol{\sigma}) = \boldsymbol{\sigma}^{T}\boldsymbol{\sigma}$ constrained to $\sigma_{1} + \ldots + \sigma_{n} = s$. This is solved by Lagrange's multipliers method. The Lagrangian function 
to optimize is then:\\
\begin{proof}
    $\displaystyle \mathcal{L}(\boldsymbol{\sigma}, \lambda) = \boldsymbol{\sigma}^{T}\boldsymbol{\sigma} - \lambda (\sum_{j=0}^{n}\sigma_{j} - s)$.\\
\\
Note the constraining condition can be abbrebiated to ${\lVert \boldsymbol{\sigma} \lVert}_{1} = s$; computing the Lagrangian gradient and equating it to zero, we get:

\begin{equation*}
    {\nabla}_{\sigma, \lambda} \mathcal{L} = 0 
    \Longleftrightarrow \begin{cases}
        \sigma = -\lambda \cdot \boldsymbol{1} \\
        {\lVert \boldsymbol{\sigma} \lVert}_{1} - s = 0.
    \end{cases}
\end{equation*}
\noindent From the first and second equations follows that $\sigma_k  = \frac{s}{n} \text{ , } \forall k \in \{ 1, \ldots ,n \}$. On the other hand, the \href{https://en.wikipedia.org/wiki/Hessian_matrix}{Bordered Hessian} has the form: \\
\begin{equation*}
    H_{\mathcal{L}}(\boldsymbol{\sigma}, \lambda) =  \begin{bmatrix}
        0 & \boldsymbol{\sigma}\\
        \boldsymbol{\sigma}^{T} & Id 
    \end{bmatrix}
\end{equation*}    
Which is obvously positively definite. Thus, the application of Lagrange's multipliers method gives a contrained minimum. \\
\end{proof}
\end{document}